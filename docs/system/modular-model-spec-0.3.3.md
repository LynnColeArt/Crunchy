
# Modular and Extensible Specification for Contextual Representation in Musical Notation Modeling

**Version 0.3.3**

## Overview

This document outlines a modular framework for encoding musical information in JSON format. The specification is designed to capture both traditional notation and performance data while maintaining format independence. This approach is intended to support mid-level developers, senior developers, and technically minded musicians who are comfortable working with structured data.

----------

## Supported Notation Formats

This specification supports a range of input formats, ensuring broad compatibility and flexibility in music processing. The following formats are currently supported:

### MusicXML

-   **Description**: A widely-used standard for representing Western musical notation.
    
-   **Strengths**:
    
    -   High level of detail, including articulations and dynamics.
        
    -   Well-suited for interoperability with notation software like Finale and Sibelius.
        
-   **Limitations**: Complex to parse due to its verbosity.
    

### MuseScore JSON

-   **Description**: A lightweight format generated by MuseScore for representing musical scores.
    
-   **Strengths**:
    
    -   Compact and human-readable.
        
    -   Easily extensible for advanced use cases.
        
-   **Limitations**: Less widely supported compared to MusicXML.
    

### MIDI

-   **Description**: A performance-focused format capturing note data, velocities, and timing.
    
-   **Strengths**:
    
    -   Universally supported by digital audio workstations (DAWs).
        
    -   Ideal for playback and performance applications.
        
-   **Limitations**: Lacks detailed notation information like articulations or dynamics.
    

### Compatibility Notes

The specification aims to ensure that conversions between these formats maintain musical fidelity wherever possible. However, users should note:

-   The framework exceeds the bounds of traditional notation formats by incorporating features inspired by the MPE (MIDI Polyphonic Expression) standard. This approach enables more expressive compositions, providing advanced possibilities for nuanced playback and performance modeling.
    
-   All data models are designed to remain backward-compatible with older formats, ensuring that users can transition seamlessly while exploring these new capabilities.
    
-   Format-specific features (e.g., glissando in MusicXML) may not translate fully to simpler formats like MIDI.    

----------

## Core Structure

### Base Score Format

The base score defines the metadata, structure, and parts of a musical composition. This representation aims to provide flexibility while maintaining clarity for potential use cases.

```
{
  "score": {
    "metadata": {
      "title": "Example Score",
      "tickResolution": 960,
      "initialTempo": 120,
      "initialTimeSignature": {
        "numerator": 4,
        "denominator": 4
      }
    },
    "structure": [
      {
        "section": "verse",
        "startMeasure": 1,
        "endMeasure": 4,
        "lyrics": [
          {
            "text": "First",
            "timing": {"measure": 1, "parts": 0}
          },
          {
            "text": "verse",
            "timing": {"measure": 1, "parts": 1}
          },
          {
            "text": "line",
            "timing": {"measure": 1, "parts": 2}
          }
        ]
      },
      {
        "section": "chorus",
        "startMeasure": 5,
        "endMeasure": 8
      }
    ],
    "parts": []
  }
}
```

----------

## Non-Western Music Representation

### Microtonal System Support

This specification extends to include microtonal systems, which allow precise pitch representations beyond the standard 12-TET (twelve-tone equal temperament). Below is an example:

```
{
  "tuningSystem": {
    "type": "microtonal",
    "baseNote": "A4",
    "frequency": 440,
    "divisions": 24,
    "intervals": [
      {"ratio": 1.0, "cents": 0},
      {"ratio": 1.0417, "cents": 70}
    ]
  }
}
```

### Cultural-Specific Articulations

This framework incorporates regionally-specific articulations to reflect diverse musical traditions. For example, **gamaka** from Carnatic music can be defined:

```
{
  "articulations": {
    "gamaka": {
      "type": "ornament",
      "style": "carnatic",
      "mpe_mapping": {
        "pitch_bend": {"range": [-50, 50], "rate": "variable"},
        "timbre": {"cc": 74, "range": [20, 100]}
      }
    }
  }
}
```

### Regional Time Concepts

The system supports time structures such as **tala** in Indian classical music, allowing for unique rhythmic frameworks:

```
{
  "rhythmicStructure": {
    "type": "tala",
    "name": "tintal",
    "beats": 16,
    "subdivisions": [4, 4, 4, 4],
    "emphasis": [1, 0, 2, 0]
  }
}
```

----------

## Instrument Definitions

### Modular Instruments

Instrument definitions offer modularity, allowing variations such as tuning, range, and timbre to be specified dynamically. These variations aim to accommodate diverse performance and notation requirements.

#### Hybrid Instruments

Hybrid instruments combine multiple sound-producing components, enabling complex performance possibilities.

#### Example: Fiddarmonica

**fiddarmonica.instrument**

```
{
  "name": "Fiddarmonica",
  "type": "hybrid.wind.string",
  "components": {
    "harmonica": {
      "type": "wind",
      "range": ["C4", "C6"],
      "mpe_parameters": {
        "breath": {
          "channel": 2,
          "cc": 74
        },
        "pressure": {
          "channel": 2,
          "cc": 73
        }
      }
    },
    "strings": {
      "type": "string",
      "tuning": ["G3", "D4", "A4", "E5"],
      "mpe_parameters": {
        "bow_pressure": {
          "channel": 3,
          "cc": 74
        },
        "bow_position": {
          "channel": 3,
          "cc": 75
        },
        "vibrato": {
          "channel": 3,
          "cc": 76
        }
      }
    }
  }
}
```

**fiddarmonica.logic**

```
{
  "behaviors": {
    "BlowWhileBowing": {
      "mpe_parameters": {
        "resonance": {
          "channel": 4,
          "cc": 77,
          "value_range": [60, 120],
          "blend": {
            "inputs": ["breath", "bow_pressure"],
            "curve": "multiplicative"
          }
        }
      }
    }
  }
}
```

----------

## Playstyles

### Definition and Use Cases

Playstyles are modifiers that alter the interpretation and playback of a musical composition. They can be applied globally, at the layer level, or to individual notes, offering varying levels of granularity.

----------

## Modifiers and Effects

### Definition

Modifiers dynamically alter an instrumentâ€™s behavior or properties during playback, while effects act on the sound output.

#### Example Modifier

```
{
  "type": "vibrato",
  "depth": 0.5,
  "blend": {
    "type": "linear",
    "duration": "1s",
    "priority": 1
  }
}
```

#### Example Effect

```
{
  "type": "reverb",
  "intensity": 0.8,
  "blend": {
    "type": "exponential",
    "duration": "2s"
  }
}
```

----------

## Unified Notes and Chords Representation

### Chord Representation

Chords are treated as grouped notes, inheriting modifiers and effects seamlessly.

#### Example

```
{
  "id": "guitar1",
  "type": "instrument",
  "instrument": "acoustic_guitar",
  "notes": [
    {
      "pitches": ["G3", "B3", "D4"],
      "timing": {"start": "1.1.0", "duration": "2.0.0"},
      "modifiers": [
        {"type": "dynamic", "value": "forte"}
      ]
    },
    {
      "pitches": ["C4", "E4", "G4"],
      "timing": {"start": "3.1.0", "duration": "2.0.0"}
    }
  ]
}
```

----------

## Timeline and Layers

### Features

-   **Absolute Positioning:** Based on measures and parts.
    
-   **Layering:** Each track or instrument exists on its own layer.
    
-   **Blending and Transitions:** Smooth transitions between effects and modifiers.
    

#### Example Timeline

```
{
  "timeline": {
    "layers": [
      {
        "id": "track1",
        "instrument": "piano",
        "notes": [
          {"start": "1.1.0", "duration": "1.0.0", "pitch": "C4"}
        ],
        "effects": [
          {"start": "1.1.0", "end": "2.0.0", "type": "reverb", "intensity": 0.7}
        ]
      },
      {
        "id": "track2",
        "instrument": "violin",
        "notes": [
          {"start": "1.2.0", "duration": "1.0.0", "pitch": "E4"}
        ],
        "modifiers": [
          {"start": "1.1.0", "end": "1.3.0", "type": "vibrato", "depth": 0.3}
        ]
      }
    ]
  }
}
```

----------

## Version History

-   **0.0.1:** Initial specification drafted, outlining the core concept, foundational structure, and primary goals for modular, extensible music notation modeling.
-   **0.3.2:** Revised notation standard from Go to JSON, realizing that compiling the data models into native machine code was probably not necessary for most musical compositions. Introduced detailed examples for modifiers, effects, timeline layering, and cultural-specific articulations. Added sections on vocal synthesis and non-Western music representation.
-   **0.3.3:** Added hybrid instrument definitions, expanded cultural-specific articulations, and refined playstyle examples. Reintroduced sections on modifiers, timeline and layers, and chord representation to align with prior iterations. Updated acknowledgments to reflect contributors.
    

----------

## Acknowledgments

This document has benefited from valuable feedback and contributions from Tim Simpson Jr., Michael Davis, Node Chomsky, Rich Fantasia, Gene The Machine, and Rachel M. Their expertise and collaboration have been instrumental in shaping this specification.

This document is designed to discuss the solution to a number of technical challenges. But it is a work in progress. Contributions of insights, technical expertise and opinions are always welcome.